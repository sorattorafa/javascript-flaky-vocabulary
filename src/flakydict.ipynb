{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Flaky Tests from JSON and returns a python dictionary\n",
    "\n",
    "2 Datasets are provided:\n",
    " - Flaky x token\n",
    " - Flaky x token+type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "from matplotlib.colors import is_color_like\n",
    "\n",
    "# DONE (P0): contar repeticao dos tokens em um flaky {value, type, quantity}\n",
    "# DONE (P0): Criar a tabela: linhas (testes) e colunas todos os tokens (de todos testes);\n",
    "\n",
    "# TODO: 3 DATASETS: \n",
    "    # DONE: - APENAS OS TOKENS\n",
    "    # DONE: - APENAS OS TOKENS + TIPOS\n",
    "        # TODO (P1): verificar a abordagem de ignorar o tipo do token ou concatenar ele com o valor do token\n",
    "    # - APENAS OS TOKENS CONFORME O SIGNIFICADO (e.g. string, url, aplicar regex)\n",
    "        # TODO (P1): verificar separadamente se vale a pena unir tokens de mesmo tipo (e.g. numérico, string, url, etc) \n",
    "            # tipos de strings : tamanho de uma tela em pixel, cores, arquivo, diretorio, formatacao, marcadores html,\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def flaky_line(df, row, df_type):\n",
    "    \"\"\"\n",
    "    receives flaky column and returns a flaky line\n",
    "    \"\"\"\n",
    "\n",
    "    new_flaky_line = {} # get  flakies with type dataset\n",
    "    new_flaky_line['id'] = row['URL']\n",
    "    for column in df.columns:\n",
    "        if(column != 'id'):\n",
    "            new_flaky_line[column] = 0\n",
    "    for column in df.columns:\n",
    "        if(column != 'id'):\n",
    "            for token in row['tokens']:\n",
    "                if(df_type == 'value'):\n",
    "                    if(column == token['value']):\n",
    "                        new_flaky_line[column] = token['quantity'] \n",
    "                elif(df_type == 'value_and_type'):\n",
    "                    if(column == token['value'] + '_' + token['type']):\n",
    "                        new_flaky_line[column] = token['quantity']\n",
    "    new_flaky_line['is_flaky'] = row['is_flaky']\n",
    "\n",
    "    return new_flaky_line\n",
    "\n",
    "def get_datasets(file_loc):\n",
    "    \"\"\"\n",
    "    receives json file and returns flaky dataframes\n",
    "    \"\"\"\n",
    "    flaky_dataset = {}\n",
    "    flaky_dataset_value_and_type = {}\n",
    "\n",
    "    with open(file_loc, encoding='utf8') as json_file:\n",
    "        data = json.load(json_file) # Opening JSON file\n",
    "\n",
    "        for row in data: # agrupando tokens iguais\n",
    "            for token in row['tokens']:\n",
    "                token['quantity'] = row['tokens'].count(token)\n",
    "            unique_list = pd.DataFrame(row['tokens']).drop_duplicates().to_dict('records')\n",
    "            row['tokens'] = unique_list\n",
    "\n",
    "        for row in data:    \n",
    "            for token in row['tokens']:\n",
    "                flaky_dataset[str(token['value'])] = []\n",
    "                flaky_dataset_value_and_type[str(token['value'] + '_' + token['type'])] = []\n",
    "\n",
    "        df = pd.DataFrame(flaky_dataset) # construção de colunas e linhas\n",
    "        df_token_and_type = pd.DataFrame(flaky_dataset_value_and_type)  # construção de colunas e linhas\n",
    "\n",
    "\n",
    "        flakies = [] # get  flakies dataset\n",
    "        flakies_value_and_type = [] # get  flakies with type dataset\n",
    "\n",
    "        for row in data:\n",
    "            flaky = flaky_line(df, row, 'value')\n",
    "            flakies.append(flaky)\n",
    "            \n",
    "            flaky_value_and_type = flaky_line(df_token_and_type, row, 'value_and_type')\n",
    "            flakies_value_and_type.append(flaky_value_and_type)\n",
    "\n",
    "        df = pd.DataFrame(flakies)\n",
    "        df_token_and_type = pd.DataFrame(flakies_value_and_type)\n",
    "        return [df,df_token_and_type]\n",
    "\n",
    "def get_string_value(token):\n",
    "    fonts = [\"'Roboto'\", \"'Arial'\", \"'Times New Roman'\", \"'Courier New'\", \"'Comic Sans MS'\", \"'Impact'\", \"'Georgia'\", \"'Palatino'\", \"'Helvetica'\", \"'Trebuchet MS'\", \"'Verdana'\"]\n",
    "    if('%' in token['value'].replace(\"'\", '')):\n",
    "        token_value = 'Percentage'\n",
    "    elif(token['value'].replace(\"'\", '') in ['x', 'y', 'z']):\n",
    "        token_value = 'Coordinate'\n",
    "    elif(\"'rgb(\" in token['value']):\n",
    "        token_value = 'Color'\n",
    "    elif(is_color_like(token['value'].replace(\"'\", ''))):\n",
    "        token_value = 'Color'\n",
    "    elif(token['value'] in fonts):\n",
    "        token_value = 'Font'\n",
    "    elif(\"'./\" in token['value']):\n",
    "        token_value = 'LocalPath'\n",
    "    elif(\"'/\" in token['value']):\n",
    "        token_value = 'Path'\n",
    "    elif(\"px'\" in token['value']):\n",
    "        token_value = 'Size Measure'\n",
    "    elif(token['value'].replace(\"'\", '') in [\"rigth\", \"left\", \"top\", \"bottom\", \"center\"]):\n",
    "        token_value = 'Direction' \n",
    "    # verificar url web http, https, ftp...\n",
    "    # is url method\n",
    "    elif(token['type'] == 'String' and 'http' in token['value'].replace(\"'\", '')):\n",
    "        token_value = 'URL'\n",
    "    elif(token['type'] == 'String' and '/api/' in token['value'].replace(\"'\", '')):\n",
    "        token_value = 'URL'\n",
    "    else:\n",
    "        return token['value']\n",
    "    return token_value\n",
    "\n",
    "def get_token_clustered_value(token):\n",
    "    # este agrupamento é ajustado para um contexto web\n",
    "    # isto pode gerar um overfiting para o dominio web\n",
    "    # argumento: esses termos sao dinamicos em diferentes contextos\n",
    "    if(token['type'] == 'Identifier'):\n",
    "        token['value'] = token['value']\n",
    "    elif(token['type'] == 'Numeric'):\n",
    "        token['value'] = token['type']\n",
    "    elif(is_date(token['value'].replace(\"'\", ''))):\n",
    "        token['value'] = 'Date'       \n",
    "    if(token['type'] == 'String'):\n",
    "        token['value'] = get_string_value(token)\n",
    "    return token['value']\n",
    "                       \n",
    "def clustered_dataset(file_loc):\n",
    "    \"\"\"\n",
    "    receives json file and returns flaky clustered dataframes\n",
    "    \"\"\"\n",
    "    clustered_dataset = {}\n",
    "    with open(file_loc, encoding='utf8') as json_file:\n",
    "        data = json.load(json_file) # Opening JSON file\n",
    "        for row in data: # agrupando tokens iguais de acordo com o contexto\n",
    "            for token in row['tokens']:\n",
    "                token['value'] = get_token_clustered_value(token)\n",
    "           \n",
    "        for row in data: # agrupando tokens iguais\n",
    "            for token in row['tokens']:        \n",
    "                token['quantity'] = row['tokens'].count(token)\n",
    "            unique_list = pd.DataFrame(row['tokens']).drop_duplicates().to_dict('records')\n",
    "            row['tokens'] = unique_list\n",
    "\n",
    "        for row in data:    \n",
    "            for token in row['tokens']:\n",
    "                clustered_dataset[str(token['value'])] = []\n",
    "      \n",
    "        df = pd.DataFrame(clustered_dataset)\n",
    "        flakies = [] # get  flakies dataset\n",
    "        \n",
    "        for row in data:\n",
    "            flaky = flaky_line(df, row, 'value')\n",
    "            flakies.append(flaky)\n",
    "            \n",
    "        df = pd.DataFrame(flakies)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 2896) (144, 2944) (144, 2504)\n",
      "(761, 2691) (761, 2692) (761, 2638)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Flaky datasets\n",
    "    flaky_tests_json = '../datasets/tests/flaky-parsed.json'\n",
    "    dfs = get_datasets(flaky_tests_json)\n",
    "    df = dfs[0]\n",
    "    df_token_and_type = dfs[1]\n",
    "    clustered_df = clustered_dataset(flaky_tests_json)\n",
    "\n",
    "    ## data\n",
    "    print(df.shape, df_token_and_type.shape, clustered_df.shape)\n",
    "    df.to_csv('../datasets/dataframes/flakies/1.csv', index=False)\n",
    "    df_token_and_type.to_csv('../datasets/dataframes/flakies/2.csv', index=False)\n",
    "    clustered_df.to_csv('../datasets/dataframes/flakies/3.csv', index=False)\n",
    "\n",
    "    \n",
    "    # Normal datasets\n",
    "    normal_tests_json = '../datasets/tests/normal-tests.json'\n",
    "    normal_dfs = get_datasets(normal_tests_json)\n",
    "    normal_df = normal_dfs[0]\n",
    "    normal_df_token_and_type = normal_dfs[1]\n",
    "    normal_clustered_df = clustered_dataset(normal_tests_json)\n",
    "\n",
    "    ## data\n",
    "    print(normal_df.shape, normal_df_token_and_type.shape, normal_clustered_df.shape)\n",
    "\n",
    "    normal_df.to_csv('../datasets/dataframes/normal/1.csv', index=False)\n",
    "    normal_df_token_and_type.to_csv('../datasets/dataframes/normal/2.csv', index=False)\n",
    "    normal_clustered_df.to_csv('../datasets/dataframes/normal/3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c37265721fe2cdac5646dd44d1cbf2722bb500abc7bebea8600b08ea20db7270"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
